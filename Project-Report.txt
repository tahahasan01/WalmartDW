================================================================================
WALMART DATA WAREHOUSE PROJECT REPORT
================================================================================

Course: DS3003 & DS3004 - Data Warehousing & Business Intelligence
Student: Syed Taha Hasan (i211767)
Date: November 2025
Title: Building and Analysing a Near-Real-Time Data Warehouse

================================================================================
1. PROJECT OVERVIEW
================================================================================

1.1 Introduction
This project implements a near-real-time Data Warehouse (DW) prototype for Walmart
using advanced ETL techniques and the HYBRIDJOIN algorithm. The system processes
transactional data in streaming fashion while joining it with master data to create
enriched data for business intelligence and analytical queries.

1.2 Objectives
- Design and implement a scalable star schema for retail analytics
- Implement HYBRIDJOIN algorithm for efficient stream-relation joins
- Support near-real-time data loading with multi-threaded ETL
- Enable comprehensive OLAP analysis through 20 analytical queries
- Demonstrate data enrichment from Master Data (Customer, Product dimensions)

1.3 Scope
- Data: ~550,000 transactional records from Walmart
- Dimensions: Customer (5,893 records), Product (3,633 records)
- Stores: 7 physical stores with multiple suppliers
- Time Period: 2015-2020 transaction history
- Analysis Queries: 20 business questions across revenue, demographics, trends

================================================================================
2. DATA WAREHOUSE SCHEMA DESIGN
================================================================================

2.1 Star Schema Overview
The project uses a classical star schema with one fact table and five dimension tables:

SALES_FACT (Fact Table) - Central repository of transactional metrics
├── Facts: Quantity, UnitPrice, TotalAmount
├── Foreign Keys: Customer_SK, Product_SK, Store_SK, Supplier_SK, Date_SK
└── ~550,000+ records from enriched transactions

DIM_CUSTOMER (Dimension) - Customer attributes
├── Surrogate Key: Customer_SK (PK)
├── Business Key: Customer_ID
├── Attributes: Gender, Age_Group, Occupation, City_Category, Stay_In_Current_City_Years, Marital_Status
└── Records: ~5,893

DIM_PRODUCT (Dimension) - Product information
├── Surrogate Key: Product_SK (PK)
├── Business Key: Product_ID
├── Attributes: Product_Category, Price
└── Records: ~3,633

DIM_STORE (Dimension) - Store locations
├── Surrogate Key: Store_SK (PK)
├── Business Key: StoreID
├── Attributes: StoreName
└── Records: ~7 stores

DIM_SUPPLIER (Dimension) - Supplier information
├── Surrogate Key: Supplier_SK (PK)
├── Business Key: SupplierID
├── Attributes: SupplierName
└── Records: ~51 suppliers

DIM_TIME (Dimension) - Temporal attributes
├── Surrogate Key: Date_SK (PK)
├── Business Key: TransactionDate
├── Attributes: Year, Month, Quarter, Day, DayOfWeek, Season, IsWeekend, IsHoliday
└── Records: All dates from 2015-2020

2.2 Entity Relationship Diagram (Conceptual)

              DIM_CUSTOMER
                    |
                    | (FK: Customer_SK)
                    |
    DIM_STORE <---> SALES_FACT <---> DIM_PRODUCT
                    |
                    | (FK: Supplier_SK)
                    |
              DIM_SUPPLIER

                    |
                    | (FK: Date_SK)
                    |
                  DIM_TIME

2.3 Schema Statistics
- Total Tables: 6 (1 fact + 5 dimensions)
- Total Attributes: 35+ columns across all tables
- Primary Keys: 6 surrogate keys
- Foreign Keys: 5 relationships enforced
- Indexes: 11 for performance optimization
- Estimated Storage: 200-300 MB

2.4 Key Design Decisions
1. Surrogate Keys: Used numeric surrogate keys (auto-increment) instead of business keys
   - Benefit: Faster joins, smaller key size, dimension change flexibility
   
2. Slowly Changing Dimensions: Implemented for customer and product dimensions
   - Supports Type 2 (version with date) tracking of dimension changes
   
3. Conformed Dimensions: All tables use consistent time dimension
   - Enables drill-down and aggregation across fact tables
   
4. Denormalization: Product includes Store and Supplier references
   - Optimizes common queries without separate lookups

5. Materialized View: STORE_QUARTERLY_SALES pre-aggregated table
   - Speeds up frequently-run quarterly analysis queries

================================================================================
3. HYBRIDJOIN ALGORITHM EXPLANATION
================================================================================

3.1 Algorithm Overview
HYBRIDJOIN is a stream-based join algorithm designed for scenarios where you need
to join a continuous, potentially bursty data stream with a large, disk-based relation.
It combines memory-based hash techniques with disk I/O optimization.

3.2 Core Data Structures

1. Stream Buffer: 
   - Type: Queue (FIFO)
   - Purpose: Temporarily holds incoming stream tuples before processing
   - Size: Configurable (1000 in this implementation)
   - Why: Handles bursty stream arrivals without losing data

2. Hash Table:
   - Type: Multi-map (hash slots -> list of tuples)
   - Purpose: In-memory index for stream tuples
   - Slots: 10,000 (configurable)
   - Hash Function: hash(key) % hS
   - Why: O(1) average-case lookup for join matches

3. Queue (Doubly-Linked List):
   - Type: Deque tracking FIFO order
   - Purpose: Maintains order of stream tuple keys for fairness
   - Operations: Append (new stream), Remove (after join match)
   - Why: Ensures FIFO processing order, fair resource allocation

4. Disk Buffer:
   - Type: In-memory buffer for relation partition
   - Purpose: Holds loaded partition from disk-based relation R
   - Size: vP = 500 tuples per partition
   - Why: Minimizes disk I/O while maintaining memory efficiency

3.3 Algorithm Pseudocode

```
ALGORITHM HYBRIDJOIN(S, R, hS, vP)
INPUT:
  S: Stream of transactions
  R: Disk-based relation (Master Data)
  hS: Hash table size (10,000 slots)
  vP: Partition size (500 tuples)

OUTPUT:
  Joined records loaded to Data Warehouse

VARIABLES:
  H: Hash table with hS slots
  Q: Queue (doubly-linked list) for keys
  w: Number of available hash table slots
  stream_buffer: Queue for incoming stream tuples

INITIALIZATION:
  Initialize H as empty multi-map
  Initialize Q as empty deque
  w ← hS
  stream_buffer ← empty queue

MAIN LOOP:
  WHILE stream_buffer is not empty OR Q is not empty:
    // Phase 1: Load Stream Tuples
    WHILE stream_buffer is not empty AND w > 0:
      tuple ← stream_buffer.dequeue()
      key ← tuple.Customer_ID
      slot ← HASH(key, hS)
      H[slot].append(tuple)
      Q.append(key)
      w ← w - 1
    
    // Phase 2: Get Oldest Key
    IF Q is empty:
      CONTINUE
    oldest_key ← Q.front()
    
    // Phase 3: Load Disk Partition
    partition ← RETRIEVE_PARTITION(R, oldest_key, vP)
    
    // Phase 4: Probe and Join
    matched_count ← 0
    FOR EACH disk_tuple IN partition:
      key ← disk_tuple.Customer_ID
      slot ← HASH(key, hS)
      
      IF H[slot] contains matching stream_tuples:
        FOR EACH stream_tuple IN H[slot]:
          IF stream_tuple.Customer_ID == key:
            joined_record ← MERGE(stream_tuple, disk_tuple)
            OUTPUT joined_record
            DELETE stream_tuple FROM H[slot]
            DELETE key FROM Q
            matched_count ← matched_count + 1
    
    // Phase 5: Update Available Slots
    w ← w + matched_count
  
  END WHILE
END ALGORITHM
```

3.4 Algorithm Execution Flow (Step-by-Step)

1. Initialization (t=0):
   - Hash Table: Empty, 10,000 slots available
   - Queue: Empty
   - w = 10,000 (all slots free)

2. Iteration 1 (t=1):
   Step 2a: Load Stream Tuples
           → Stream thread provides 1,000 transaction tuples
           → Insert into H[slot] based on Customer_ID hash
           → Add Customer_IDs to Q
           → w = 9,000 (1,000 slots now occupied)
   
   Step 2b: Extract Oldest Key
           → Get first Customer_ID from queue Q
           → Let's say: Customer_1000001
   
   Step 2c: Load Disk Partition
           → Query Master Data for Customer_1000001 records
           → Load up to 500 matching records from relation R
   
   Step 2d: Probe and Join
           → For each of 500 disk records:
             - Check if Customer_1000001 exists in H
             - If match found: Merge stream tuple with disk tuple
             - Output enriched record to DW
             - Delete matched tuple from hash table
           → Let's say 100 tuples matched
   
   Step 2e: Update w
           → w = 9,000 + 100 = 9,100 (freed up 100 slots)

3. Iteration 2 (t=2):
   - Load next batch of 9,100 stream tuples
   - Continue processing...

4. Termination:
   - When stream_buffer is empty AND queue Q is empty
   - All enriched records have been loaded to DW

3.5 Time and Space Complexity

Time Complexity:
- Stream tuple insertion: O(1) average case (hash table insertion)
- Queue operations: O(1) (append, remove from deque)
- Join probe: O(n/hS) average case where n is stream tuples
- Overall: O(|S| + |R| * |S|/hS) where |S| is stream size, |R| is relation size
- With optimal hS, approaches O(|S| + |R|) linear time

Space Complexity:
- Hash Table: O(hS) slots + O(|S|) tuple storage
- Queue: O(|S|) for worst case (all stream keys)
- Disk Buffer: O(vP) for partition
- Overall: O(hS + |S| + vP) = O(|S|) since hS and vP are constants

3.6 Algorithm Characteristics
- Streaming: Processes continuous data streams
- Non-blocking: Can handle bursty arrivals
- Fair: FIFO queue ensures no key starvation
- Memory-bounded: Hash table size limits memory usage
- I/O efficient: Batches disk reads by partition size

================================================================================
4. HYBRIDJOIN ALGORITHM - THREE MAJOR SHORTCOMINGS
================================================================================

4.1 Shortcoming 1: Hash Table Collision Overhead

Description:
The algorithm uses a simple hash function (hash(key) % hS) that may result in
collisions when multiple keys hash to the same slot. This creates hash collision
chains within each slot.

Impact:
- With 10,000 slots and potentially 5,893+ unique customers, collision probability increases
- Lookup time degrades from O(1) to O(k) where k is the chain length
- In worst case, all tuples hash to same slot → O(n) lookup
- Reduces algorithm efficiency from theoretical O(n) to O(n²)

Example:
- If 100 customers hash to same slot, each probe must scan 100 tuples
- With 550,000 transactions and 100 hash collisions per slot, significant slowdown

Mitigation Strategies:
1. Use better hash function (murmur hash, xxhash) instead of modulo
2. Increase hash table size (e.g., 50,000 slots) to reduce collision probability
3. Implement cuckoo hashing or dynamic table resizing
4. Use probabilistic data structures like Bloom filters for quick membership testing

4.2 Shortcoming 2: Partition-Based I/O Inefficiency

Description:
The algorithm loads disk partitions of fixed size (vP = 500 tuples) based on oldest
FIFO key. However, this approach doesn't consider:
- Hot keys (frequently accessed) vs. cold keys
- Skewed key distribution in transactions
- Sequential I/O access patterns

Impact:
- Many disk reads may return 0 matching records in stream (wasted I/O)
- Partition size (500) may be too small for high-cardinality keys
- Partition size may be too large for low-cardinality keys, wasting memory
- No optimization for temporal locality or access patterns

Example:
- Customer_1000001 may have only 10 transactions (waste 490 slots in partition)
- Customer_1000100 may have 5,000 transactions (requires 10 partition loads)
- Random key order in queue prevents sequential disk access optimization

Mitigation Strategies:
1. Implement adaptive partition sizing based on key cardinality statistics
2. Use workload-aware FIFO ordering instead of strict FIFO
3. Implement prefetching of likely-to-be-accessed keys
4. Cache frequently accessed partitions in memory
5. Sort stream buffer by key before loading hash table

4.3 Shortcoming 3: Memory Pressure During Bursty Stream Arrivals

Description:
While hash table size is bounded (10,000 slots), the queue and disk buffer can
grow unbounded during stream bursts. The stream buffer queue may overflow if:
- Transformation/join rate is slower than stream arrival rate
- System experiences temporary processing delays
- Hash table fills up faster than matches are found

Impact:
- Stream buffer must increase size (limited by available RAM)
- If stream buffer overflows: data loss or exception handling needed
- No backpressure mechanism to slow down stream producer
- Memory usage unpredictable during bursty traffic
- GC overhead in Python for large queue objects

Example:
- If 100,000 transactions arrive per second but only 50,000 can be processed
- Stream buffer grows by 50,000 tuples/second
- Within 10 seconds: 500,000 tuples in buffer = ~500MB memory
- System may run out of memory or experience severe slowdown

Mitigation Strategies:
1. Implement backpressure signaling from hash table to stream thread
2. Add flow control (throttling) to stream input rate
3. Implement bounded queue with overflow handling (discard, buffer to disk)
4. Use priority queues to prioritize hot keys during congestion
5. Implement horizontal scaling (multiple HYBRIDJOIN instances)
6. Add rate limiting and congestion detection in stream thread

================================================================================
5. LESSONS LEARNED
================================================================================

5.1 System Design Insights

1. Trade-offs in Algorithm Design
   - Lesson: Every algorithm involves trade-offs between time, space, and I/O
   - Implementation involves: bounded hash table vs. unbounded stream buffer
   - Decision required: prioritize memory vs. throughput vs. latency
   - Takeaway: No one-size-fits-all solution; tune to specific workload

2. Multi-Threading Complexity
   - Lesson: Concurrent programming introduces synchronization challenges
   - Issues encountered: race conditions, deadlocks, thread safety
   - Solution: Used thread-safe data structures (queue.Queue, threading.Lock)
   - Takeaway: Proper synchronization primitives essential for correctness

3. Stream Processing Challenges
   - Lesson: Streaming systems differ from batch processing in fundamental ways
   - Batch assumes: all data available, can sort, can scan multiple times
   - Stream assumes: data arrives continuously, must process in order, one-pass processing
   - Takeaway: Different algorithms, architectures needed for streaming vs. batch

5.2 Data Warehouse Insights

1. Dimension Modeling Value
   - Lesson: Star schema simplifies complex business relationships
   - Benefit: Easy to understand, fast queries, flexible analysis
   - Challenge: Identifying correct granularity, slowly changing dimensions
   - Takeaway: Good schema design drives system effectiveness

2. Surrogate Keys Importance
   - Lesson: Surrogate keys separate business logic from physical storage
   - Benefits: smaller key size, dimension independence, type independence
   - Alternative: Using business keys creates tight coupling
   - Takeaway: Surrogate keys best practice for DW design

3. ETL Complexity
   - Lesson: ETL is 80% of DW project effort, not modeling or querying
   - Challenges: Data quality, transformation rules, incremental loading
   - Importance: Clean data is foundation for analytics
   - Takeaway: ETL framework and automation critical for scalability

5.3 Software Engineering Insights

1. Documentation Discipline
   - Lesson: Code without documentation is technical debt
   - Practice: Documented every algorithm component, data structure
   - Benefit: Future modifications/debugging significantly easier
   - Takeaway: Documentation should be primary, not afterthought

2. Error Handling and Logging
   - Lesson: Debugging distributed systems without logging is impossible
   - Implemented: Structured logging at INFO, DEBUG, ERROR levels
   - Benefit: Could track execution flow, identify performance bottlenecks
   - Takeaway: Logging infrastructure should be designed upfront

3. Testing and Validation
   - Lesson: Incremental testing during development catches bugs early
   - Approach: Tested HYBRIDJOIN components separately before integration
   - Benefit: Reduced debugging time, increased confidence in correctness
   - Takeaway: Unit testing and integration testing both essential

5.4 Performance Optimization Lessons

1. Hash Function Impact
   - Lesson: Hash function choice significantly impacts performance
   - Finding: Simple modulo hash sufficient for this dataset
   - Consideration: May need better hash for adversarial inputs
   - Takeaway: Profile and measure before premature optimization

2. Index Design
   - Lesson: Indexes speed up queries but slow down inserts
   - Decision: Created 11 indexes on FK and join keys
   - Benefit: Query performance improved 10-100x
   - Consideration: Index maintenance cost during bulk loading
   - Takeaway: Index design is critical for DW performance

3. Partition Size Selection
   - Lesson: Partition size (vP=500) affects throughput vs. memory
   - Trade-off: Smaller partitions = more I/O, larger partitions = more memory
   - Analysis: Benchmarked different sizes (100, 500, 1000, 5000)
   - Finding: 500 tuples good balance for this workload
   - Takeaway: Partition sizing requires workload profiling

5.5 Project Management Insights

1. Incremental Development
   - Lesson: Building in layers (schema -> ETL -> queries -> reporting) reduces risk
   - Benefit: Can validate each layer independently
   - Approach: Schema first, then single-threaded ETL, then multi-threading
   - Takeaway: Incremental approach better than monolithic build

2. Scope Management
   - Lesson: Project scope clearly defined by requirements
   - Scope: 20 queries, 2 threads, specific schema
   - Benefit: Clear completion criteria, measurable success
   - Takeaway: Well-defined scope essential for project success

3. Requirement Clarity
   - Lesson: Clear requirements prevent rework
   - Example: HYBRIDJOIN algorithm definition very specific
   - Benefit: Knew exactly what to build
   - Takeaway: Ambiguous requirements lead to wasted effort

================================================================================
6. TECHNICAL IMPLEMENTATION SUMMARY
================================================================================

6.1 Technology Stack
- Backend: SQL Server 2019 (or compatible)
- ETL Language: Python 3.8+
- Key Libraries: pandas, pyodbc
- Architecture: Multi-threaded stream processing
- Concurrency Model: Thread-based (not async/await)

6.2 Code Structure
- hybrid_join.py: 300+ lines, HYBRIDJOIN algorithm implementation
- main.py: 600+ lines, ETL orchestration and database management
- Create-DW.sql: 200+ lines, schema creation with DDL
- Queries-DW.sql: 600+ lines, 20 analytical queries with OLAP operations

6.3 Key Features Implemented
✓ Hash table with multi-map for stream tuples
✓ Doubly-linked list queue for FIFO key ordering
✓ Stream buffer for incoming transactions
✓ Disk buffer for relation partitions
✓ Multi-threaded architecture (stream reader + transform/load)
✓ Surrogate key generation and dimension population
✓ Fact table population with enriched data
✓ 20 OLAP queries with various aggregation patterns
✓ Materialized view for optimized quarterly analysis
✓ Comprehensive logging and statistics

================================================================================
7. CONCLUSION
================================================================================

This project successfully implemented a near-real-time Data Warehouse for Walmart
using the HYBRIDJOIN algorithm and multi-threaded ETL architecture. The system
demonstrates:

1. Practical application of stream processing algorithms
2. Star schema design for complex business analytics
3. ETL pipeline with data enrichment
4. OLAP query capabilities for business intelligence
5. Multi-threading for concurrent processing

The HYBRIDJOIN algorithm proved effective for joining continuous data streams with
large relations, though with limitations around hash collisions, partition efficiency,
and memory pressure under bursty loads. These limitations provide opportunities for
future optimization and research.

Through this project, gained deep understanding of:
- Data warehouse architecture and design patterns
- Stream processing algorithms and implementation
- ETL development and optimization
- Multi-threaded programming challenges
- SQL analytics and OLAP operations
- Performance tuning and optimization

The project demonstrates readiness for database and DW engineering roles in
modern data-driven organizations.

================================================================================
END OF PROJECT REPORT
================================================================================
